# 深度增强学习用于基于异构off-policy更新的机械臂

> _Gu, S., Holly, E., Lillicrap, T. and Levine, S., 2016. Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. arXiv preprint arXiv:1610.00633_.
>
> 强化学习有望在较少人工干预的情况下帮助机器学得大量的行为技巧。但是**实际机器人在应用RL的时候，常常需要折中学习过程的自动化程度**，比如人为设计的策略或者人类辅助的验证（如选择策略或值函数的近似表达），**以争取获得更短的训练时间**，这一点在实际物理系统中很重要。
>
> 深度增强学习通过训练通用目的的神经网络策略（general-purpose neural network policies）可以缓解这种限制，但是由于直接的深度增强学习算法显著的**高复杂的采样**，导致其目前的**应用仅限于仿真环境和完成一些简单的任务**。

这篇文章中，作者采用**基于off-policy训练深度Q-functions的深度增强学习算法**[^1，2]可以解决复杂的三维抓取动作，并且可以学到可以高效地在实体机器人上训练的**深度神经网络策略**。此外，通过将算法在多台机器人上同时运行，异步地推动策略更新，可以进一步缩短训练时间。

实验验证了算法可以在仿真环境中和真实机器人开门动作中学到大量3D抓取技巧，这些都不需要先验知识或人为设计的表达。

### Introduction

最近提出的**基于off-policy训练深度Q-functions的深度增强学习算法**[^1，2]，可以在不需要示教的前提下，拓展到复杂的机械臂抓取策略上，只采用\[不需要与任务相关的特殊知识的\]**general-purpose neural network policies**即可。

最近提出的off-policy deep Q-function based deep reinforcement learning algorithm，如Deep Deterministic Policy Gradient algorithm（DDPG[^1]）和Normalized Advantage Function algorithm（NAF[^2]）,可以缩短训练时间以用于实际机器人。

**创新点**：在多台机器人上验证了采用作者提出的采用了并行NAF算法的异步深度学习算法。包括1.NAF算法的异步变化；2.将算法推广到实际机器人上，使其进行高效采样的训练。此外，这还是第一次在没有人类示教或先验知识基础上的全自主开门。

### Related Work

### Background

在这些实验中，我们给机器人的任务是尝试将它们的机器臂移动到目标位置，或移动到某个位置打开一扇门。每个机器人都配置了一个神经网络副本，这让它们可以评估在一个给定状态中采取一个给定动作的价值。通过查询这个网络，该机器人可以快速决定在这个世界中哪些行动可能是值得采用的。当一个机器人执行动作时，我们给其所选择的动作加入了噪声，从而使其得到的行为有时候可能会比之前所观察到的好一点，当然有时候也会差一点。这让每个机器人都可以探索完成一个任务的不同方法。这些机器人会记录下这些动作，然后它们的行为和最终的结果会被发送到中央服务器。该服务器会使用所有机器人的经历，并使用它们来迭代式地改进用于评估不同状态和动作值的神经网络。

我们部署的这个无模型算法会同时检查好的和坏的经历，并将它们提炼融入到一个新的网络中——这个网络更擅长理解动作和成功之间的关联方式。然后，每隔一段时间，每个机器人都会从该服务器获得一个更新网络的副本并开始使用新网络的信息执行动作。因为更新后的网络在评估世界中动作的价值上的表现会好一点，所以这些机器人会有更好的表现。然后不断重复这种周期以获得在该任务上持续提升。在下面的视频中，一个机器人正在探索开门的任务：[https://www.youtube.com/watch?v=iiD3Klvm96s。](https://www.youtube.com/watch?v=iiD3Klvm96s。)

几个小时的练习之后，机器人共享了它们学习实现目标（接触把手然后拉动以打开门）的原始经历。在开门的这个案例中，机器人没有构建世界的确切模型就学会了处理拉钩和门把手之间接触的复杂物理作用，结果如视频所示：[https://www.youtube.com/watch?v=R2kEi\_KKSsA](https://www.youtube.com/watch?v=R2kEi_KKSsA)

---

1. Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D., 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.

2. Gu, S., Lillicrap, T., Sutskever, I. and Levine, S., 2016. Continuous deep q-learning with model-based acceleration. arXiv preprint arXiv:1603.00748.

3. 


