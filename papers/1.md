# 深度增强学习用于基于异步off-policy更新的机械臂

> 强化学习有望在较少人工干预的情况下帮助机器学得大量的行为技巧。但是**实际机器人在应用RL的时候，常常需要折中学习过程的自动化程度**，比如人为设计的策略或者人类辅助的验证（如选择策略或值函数的近似表达），**以争取获得更短的训练时间**，这一点在实际物理系统中很重要。
>
> 深度增强学习通过训练通用目的的神经网络策略（general-purpose neural network policies）可以缓解这种限制，但是由于直接的深度增强学习算法显著的**高复杂的采样**，导致其目前的**应用仅限于仿真环境和完成一些简单的任务**。

这篇文章中，作者采用**基于off-policy训练深度Q-functions的深度增强学习算法**[^1，2]可以解决复杂的三维抓取动作，并且可以学到可以高效地在实体机器人上训练的**深度神经网络策略**。此外，通过将算法在多台机器人上同时运行，异步地推动策略更新，可以进一步缩短训练时间。

实验验证了算法可以在仿真环境中和真实机器人开门动作中学到大量3D抓取技巧，这些都不需要先验知识或人为设计的表达。

### Introduction

最近提出的**基于off-policy训练深度Q-functions的深度增强学习算法**[^1，2]，可以在不需要示教的前提下，拓展到复杂的机械臂抓取策略上，只采用\[不需要与任务相关的特殊知识的\]**general-purpose neural network policies**即可。

最近提出的off-policy deep Q-function based deep reinforcement learning algorithm，如Deep Deterministic Policy Gradient algorithm（DDPG[^1]）和Normalized Advantage Function algorithm（NAF[^2]）可以缩短训练时间以用于实际机器人。

**创新点**：在多台机器人上验证了采用作者提出的采用了并行NAF算法的异步深度学习算法。包括1.NAF算法的异步变化；2.将算法推广到实际机器人上，使其进行高效采样的训练。此外，这还是第一次在没有人类示教基础上的全自主开门。



---

1. Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D., 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.

2. Gu, S., Lillicrap, T., Sutskever, I. and Levine, S., 2016. Continuous deep q-learning with model-based acceleration. arXiv preprint arXiv:1603.00748.

3. 


