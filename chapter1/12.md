![](/assets/Screenshot from 2017-06-10 12:43:17.png)

这个算是蛮实用的，因为每次规划完执行的过程中会遇到环境变化的问题，这就需要在执行过程中重新规划。重新规划的路线与之前的路线是连接的，而不是中间停下来重新走。上面是RSS在2016年的研究，感兴趣的可以了解下。

![](/assets/Screenshot from 2017-06-10 12:44:20.png)

理想状态下机器人在运动规划下直接端一杯水到一个地方就行了，但实际情况下这个过程是有动力学在里面，如果不做任何处理，这个杯子会掉。所以，在考虑了动力学之后，重新进行运动规划，这时候杯子才不会掉。这个问题还是比较简单的，因为你只需要把它变成一个约束就好了。

![](/assets/Screenshot from 2017-06-10 12:45:17.png)

这个问题是存在的，因为在规划的时候会跟环境接触，例如这个机器人攀爬杆子然后落地，涉及到整个身体动力学跟你身体运动的协调过程，这个工作是MIT计算机科学与人工智能实验室在2014年的实验。接触动力学比传统的单体动力学复杂很多，因为我们不知道它接触的碰撞摩擦力这些不好建模。

**运动规划+任务规划**

![](/assets/Screenshot from 2017-06-10 12:46:57.png)

运动规划是指我给你一个大任务，你自动生成一些小任务。这是IROS在2016年的一个工作，它的目标是让机器人到达对面这个点，而它的路径被障碍物挡住了，这个时候把运动规划加进来，从更高一个空间维度去求解这个问题。第一步，它把这个桌子往前推，发现桌子推不动的时候对任务进行重规划，然后规划到去推这个桌子，然后发现执行的效果与预计的不一样，所以它又生成新的任务，然后它拉开桌子之后就走到了对面实现了工作。这只是一个很简单的demo，但实际上生活中会遇到很多这样的问题，比如我想从这个房间到另一个房间，而门是关着的，这个时候就需要把门打开。所以说，不是要给机器人生成很多子任务，而是一个大任务，未来服务机器人想要做好这块是必须要做的。



实用化

![](/assets/Screenshot from 2017-06-10 16:23:52.png)

大部分时间大家都用在了实用化上，虽然说只要有足够时间它一定能求解出来，但实际情况下我们不可能给它无限的时间。另外RRT这些算法生成轨迹很奇怪， 你可以看右边这个视频，只是让它敲这个东西它要画一大圈，所以这也是一个问题，就是怎么优化它的轨迹。所以需要将研究领域好的算法往工业领域推，目前两者之间是存在很大缺口的。

![](/assets/Screenshot from 2017-06-10 16:27:51.png)

这个工作是想办法把旧的轨迹给用起来，通过人工的方式指定一个运动微元，也就是原始轨迹，等到了新的环境后再进行改变。当然，这个爬楼梯的过程，环境和动作基本上都相同，所以可以在这个微元的基础上进行改变。首先，通过变形的工作拉到现在起始点位置，部分起始点会重合，然后对这些新起始点进行重复利用，它会形成一个好的轨迹。这个工作是Hauser et al在2008年发布的论文。现在存在的问题是运动微元必须由人工来指定，所以研究方向是由系统自动生成运动微元。

![](/assets/Screenshot from 2017-06-10 16:29:12.png)

这是之前做的一个内容，比较简单但在相对固定的环境比较好用。大概原理就是根据人工示教的路径，通过高斯混合模型（GMM）对可行C空间进行建模，之后在这个GMM-C空间内进行规划。这个方法有点类似Learning From Demonstration 的工作，但我只用了它们前面一半的步骤，后面一半还是采用采样的方法。

![](/assets/Screenshot from 2017-06-10 16:30:28.png)

这个是我针对加工过程做的另一个工作。我们在工业领域用机器人往往期望的不是整个机械臂的动作，而只是末端的动作。假设我要抛光一个面，首先我要对末端进行规划，用CAD模型就可以计算实现；得到路径后发给机器人，之后直接求逆解或者用雅克比迭代过去。当然，这种方法大部分时候够用，但有时候也会遇到奇异点或者碰到障碍物。我就是针对这个七轴的机械臂，利用它的一个冗余自由度进行规划。因为末端是固定的轨迹，这个时候，只要找到冗余自由度对应的C空间流形，我们就可以在这么一个低维（2维）流形内进行很快速的规划，实现末端固定轨迹，且关节避障避奇异。

**深度强化学习DRL**

![](/assets/Screenshot from 2017-06-10 16:32:53.png)

假设深度学习做运动规划，那么它进行一次运动规划的时间就是一次网络正向传播时间，这个时间非常短的，所以只要网络训练好后，运动规划需要耗费很长时间的问题就没有了。目前这块也有一些这方面的研究，上面左边图是用深度学习玩游戏，Nature上的一篇论文，效果比人还厉害；右上角是谷歌用深度学习来开门；右下角就是AlphaGo下围棋了。这个是很有意思的，它也是运动规划和控制的问题，但它是用网络来做的映射。

* 在给定信息满足系统状态可观性的前提下，CNN强大的环境理解能力（observe→state）
* RL可以进行路径规划 
  * 通过value Iteration等方式建立表格（s→a，v）
  * 用神经网络NN拟合这个映射（s→a，v）
* DQN

       ![](/assets/Screenshot from 2017-06-10 16:36:52.png)

问题

![](/assets/Screenshot from 2017-06-10 16:38:14.png)

![](/assets/Screenshot from 2017-06-10 16:39:12.png)

![](/assets/Screenshot from 2017-06-10 16:41:20.png)





